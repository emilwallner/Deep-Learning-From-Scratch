{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#source: https://github.com/llSourcell/linear_regression_live/blob/master/demo.py\n\n# A list of sublists each of which has two elements;\n# the first being the price of wheat/kg and\n# the second being the average price of bread.\nprice_wheat_bread = [[0.5,5],\n                     [0.6,5.5],\n                     [0.8,6],\n                     [1.1,6.8],\n                     [1.4,7]\n                    ]\n\ndef step_gradient(b_current, \n                  m_current, \n                  points, \n                  learningRate):\n    \"\"\"\n    For a given y-intercept, b, and slope, m, and set of points, and learning rate, \n    produce the step size of the gradient needed for changing those coefficients, b and m.\n    \"\"\"\n    b_gradient = 0 # Initialize change in slope to naught, i.e. no change.\n    m_gradient = 0 # Initialize change in y-intercept to naught, i.e. no change. \n    N = float(len(points)) # The number of pairs of points in our list of coordinates.\n    for i in range(0, len(points)): # Sweep over the index of every pair of points in the list of coordinates.\n        x = points[i][0] # Select the first number in the ith pair.\n        y = points[i][1] # Select the second number in the ith pair.\n        # How quickly does the squared error change with respect to the y-intercept?\n        # The negative sign means that the gradient points in the direction of decrease of error, instead of increase.\n        b_gradient += -(2/N) * (y - ((m_current * x) + b_current)) # Notice the two comes down based on the power rule of derivatives.\n        # How quickly does the error change with respect to the slope?\n        m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current)) # Notice the x comes out based on the rule of derivatives.\n    # Based on how large of steps you chose to to increment, the gradients (or negative of the slopes) are now used to adjust the old values.\n    new_b = b_current - (learningRate * b_gradient) # I am not sure why there is a negative sign.\n    new_m = m_current - (learningRate * m_gradient) # \n    return [new_b, new_m]\n\ndef gradient_descent_runner(points, \n                            starting_b, \n                            starting_m, \n                            learning_rate, \n                            num_iterations):\n    \"\"\"\n    Given a set of points, and initial values of b and m, and a learning rate, and number of iterations,\n    produce a...what?\n    \"\"\"\n    b = starting_b\n    m = starting_m\n    for i in range(num_iterations): # How ever many times you want to run the gradient descent, that's how many increments will change your coefficients, m and b, to readjust the predicting line, with successively less and less error.\n        b, m = step_gradient(b, m, points, learning_rate)\n    return [b, m]\n\n# Let's produce the best m and b values, given initial choices of m as 12, b as 42, \n# and the increment learning step as .01, over 10000 iterations.\n# The goal being to end up with ideal, or at least very accurate, coefficients.\ngradient_descent_runner(price_wheat_bread, 12, 42, 0.01, 10000) \n","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"[4.107202463019789, 2.2190814997453208]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Apparently the ideal value of b (y-intercept) is 4.1 and of m (slope) is 2.2"},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.13"}},"nbformat":4,"nbformat_minor":1}